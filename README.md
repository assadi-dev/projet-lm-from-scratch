# ğŸ§  LLM From Scratch : Projet de ModÃ¨le de Langage (Architecture Transformer)

Ce projet vise Ã  implÃ©menter entiÃ¨rement un modÃ¨le de langage (LLM) de type **Decoder-only** (similaire Ã  GPT) Ã  partir de zÃ©ro, en utilisant **PyTorch**.

## ğŸš€ Vue d'ensemble

Le projet couvre toute la chaÃ®ne de production d'un LLM :
- **Architecture** : ImplÃ©mentation complÃ¨te des blocs Transformer (Attention, Feed-Forward, Normalisation).
- **Tokenisation** : Gestion des diffÃ©rents types de tokeniseurs.
- **EntraÃ®nement** : Scripts pour le prÃ©-entraÃ®nement et le fine-tuning.
- **InfÃ©rence** : GÃ©nÃ©ration de texte optimisÃ©e utilisant le masquage causal.

## ğŸ—ï¸ Architecture Technique

Le modÃ¨le suit une architecture **Decoder-only Transformer** moderne, incluant :

- **Embeddings** : Support pour les embeddings de tokens et l'encodage positionnel (SinusoÃ¯dal ou RoPE).
- **Multi-Head Self-Attention** : ImplÃ©mentation avec masque causal pour empÃªcher le modÃ¨le de regarder les tokens futurs.
- **Feed-Forward Blocks** : Utilisation de GELU ou SwiGLU pour une meilleure convergence.
- **Pre-LayerNorm** : Normalisation avant chaque sous-bloc (recommandÃ© pour la stabilitÃ© de l'entraÃ®nement).
- **Inference** : MÃ©canismes de gÃ©nÃ©ration auto-rÃ©gressive.

## ğŸ“ Structure du Projet

```text
.
â”œâ”€â”€ checkpoints/       # Sauvegarde des poids du modÃ¨le
â”œâ”€â”€ config/            # Fichiers de configuration YAML
â”œâ”€â”€ data/              # Dossiers pour les donnÃ©es brutes et traitÃ©es
â”œâ”€â”€ logs/              # Logs de Tensorboard et W&B
â”œâ”€â”€ notebooks/         # ExpÃ©rimentations interactives
â”œâ”€â”€ scripts/           # Utilitaires de traitement de donnÃ©es
â”œâ”€â”€ src/               # Code source principal
â”‚   â”œâ”€â”€ model/         # DÃ©finition de l'architecture Transformer
â”‚   â”œâ”€â”€ data/          # Chargeurs de donnÃ©es (DataLoaders)
â”‚   â”œâ”€â”€ training/      # Boucles d'entraÃ®nement
â”‚   â””â”€â”€ inference/     # Scripts de gÃ©nÃ©ration de texte
â”œâ”€â”€ README.md          # Documentation principale
â”œâ”€â”€ architecture.md    # Guide technique dÃ©taillÃ©
â”œâ”€â”€ requirements.txt   # DÃ©pendances Python
â””â”€â”€ setup_project.bat  # Script d'initialisation
```

## ğŸ› ï¸ Installation et Configuration

### PrÃ©-requis
- Python 3.8+
- PyTorch 2.0+

### Installation (Windows)

1. **Initialiser la structure** (si ce n'est pas dÃ©jÃ  fait) :
   ```bash
   setup_project.bat
   ```

2. **CrÃ©er l'environnement virtuel** :
   ```bash
   python -m venv venv
   .\venv\Scripts\activate
   ```

3. **Installer les dÃ©pendances** :
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“ˆ Utilisation

1. **PrÃ©paration des donnÃ©es** : Placez vos fichiers `.txt` dans `data/raw/`.
2. **EntraÃ®nement** : Utilisez les scripts dans `src/training/`.
3. **GÃ©nÃ©ration** : Testez le modÃ¨le avec les scripts dans `src/inference/`.
4. **Chat Interactif** : DÃ©marrez une session de chat avec le modÃ¨le :
   ```bash
   python src/chat.py
   ```

## ğŸ“š Ressources
- `architecture.md` : Guide dÃ©taillÃ© sur les mathÃ©matiques et l'implÃ©mentation des composants.
- `model_template.py` : ModÃ¨le de base pour l'implÃ©mentation.

---
*Projet dÃ©veloppÃ© dans le cadre d'un apprentissage approfondi des Transformers.*
