"""
chat.py - Interface terminal pour discuter avec ton LLM

Usage:
    python chat.py                     # Charge le modÃ¨le franÃ§ais par dÃ©faut
    python chat.py --model english     # Charge le modÃ¨le anglais
    python chat.py --model chemin/vers/model.pt  # Charge un modÃ¨le spÃ©cifique
"""

import torch
import os
import sys
import argparse
from pathlib import Path

# Ajouter les chemins
src_path = os.path.dirname(os.path.abspath(__file__))
sys.path.append(src_path)
sys.path.append(os.path.join(src_path, "model"))
sys.path.append(os.path.join(src_path, "data"))
sys.path.append(os.path.join(src_path, "inference"))

from gpt import GPT
from tokenizer import Tokenizer
from generate import TextGenerator


class TerminalChat:
    """
    Interface de chat dans le terminal.
    """
    
    def __init__(self, model_path: str, device: str = "cuda"):
        self.device = device
        self.tokenizer = Tokenizer()
        
        # Charger le modÃ¨le
        print(f"ğŸ”„ Chargement du modÃ¨le: {model_path}")
        
        self.model = GPT(
            vocab_size=self.tokenizer.vocab_size,
            d_model=256,
            n_heads=8,
            n_layers=6,
            d_ff=1024,
            max_seq_len=256,
            dropout=0.0  # Pas de dropout en infÃ©rence
        )
        
        # Charger les poids
        state_dict = torch.load(model_path, map_location=device)
        
        # GÃ©rer les deux formats de sauvegarde
        if 'model_state_dict' in state_dict:
            self.model.load_state_dict(state_dict['model_state_dict'])
        else:
            self.model.load_state_dict(state_dict)
        
        self.model.to(device)
        self.model.eval()
        
        # GÃ©nÃ©rateur
        self.generator = TextGenerator(self.model, self.tokenizer, device)
        
        # ParamÃ¨tres par dÃ©faut
        self.temperature = 0.8
        self.top_k = 50
        self.max_tokens = 100
        
        print("âœ… ModÃ¨le chargÃ©!")
    
    def generate(self, prompt: str) -> str:
        """GÃ©nÃ¨re du texte Ã  partir d'un prompt."""
        return self.generator.generate(
            prompt,
            max_new_tokens=self.max_tokens,
            temperature=self.temperature,
            top_k=self.top_k
        )
    
    def generate_stream(self, prompt: str):
        """GÃ©nÃ¨re du texte en streaming (token par token)."""
        return self.generator.generate_stream(
            prompt,
            max_new_tokens=self.max_tokens,
            temperature=self.temperature,
            top_k=self.top_k
        )
    
    def print_help(self):
        """Affiche l'aide."""
        help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ¤– COMMANDES DISPONIBLES                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  /help           - Afficher cette aide                       â•‘
â•‘  /temp <valeur>  - Changer la tempÃ©rature (ex: /temp 0.9)    â•‘
â•‘  /topk <valeur>  - Changer le top_k (ex: /topk 40)           â•‘
â•‘  /tokens <n>     - Changer le nombre de tokens (ex: /tokens 150) â•‘
â•‘  /settings       - Afficher les paramÃ¨tres actuels           â•‘
â•‘  /stream         - Activer/dÃ©sactiver le mode streaming      â•‘
â•‘  /clear          - Effacer l'Ã©cran                           â•‘
â•‘  /quit           - Quitter                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ’¡ Sinon, tape simplement ton texte et appuie sur EntrÃ©e    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        print(help_text)
    
    def print_settings(self):
        """Affiche les paramÃ¨tres actuels."""
        print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     âš™ï¸  PARAMÃˆTRES ACTUELS      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TempÃ©rature : {self.temperature:<16} â”‚
â”‚  Top-K       : {self.top_k:<16} â”‚
â”‚  Max tokens  : {self.max_tokens:<16} â”‚
â”‚  Device      : {self.device:<16} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    
    def run(self):
        """Lance l'interface de chat."""
        
        # En-tÃªte
        print("\n" + "=" * 60)
        print("ğŸ¤– LLM FROM SCRATCH - MODE TERMINAL")
        print("=" * 60)
        print("Tape /help pour voir les commandes disponibles.")
        print("Tape /quit pour quitter.")
        print("=" * 60 + "\n")
        
        streaming = False
        
        while True:
            try:
                # Prompt utilisateur
                user_input = input("ğŸ“ Toi: ").strip()
                
                # Ignorer les entrÃ©es vides
                if not user_input:
                    continue
                
                # Commandes spÃ©ciales
                if user_input.startswith("/"):
                    parts = user_input.split()
                    cmd = parts[0].lower()
                    
                    if cmd == "/quit" or cmd == "/exit" or cmd == "/q":
                        print("\nğŸ‘‹ Ã€ bientÃ´t!")
                        break
                    
                    elif cmd == "/help":
                        self.print_help()
                    
                    elif cmd == "/settings":
                        self.print_settings()
                    
                    elif cmd == "/clear":
                        os.system('cls' if os.name == 'nt' else 'clear')
                    
                    elif cmd == "/stream":
                        streaming = not streaming
                        status = "activÃ©" if streaming else "dÃ©sactivÃ©"
                        print(f"  Mode streaming {status}")
                    
                    elif cmd == "/temp" and len(parts) > 1:
                        try:
                            self.temperature = float(parts[1])
                            print(f"  TempÃ©rature â†’ {self.temperature}")
                        except:
                            print("  âŒ Valeur invalide (ex: /temp 0.8)")
                    
                    elif cmd == "/topk" and len(parts) > 1:
                        try:
                            self.top_k = int(parts[1])
                            print(f"  Top-K â†’ {self.top_k}")
                        except:
                            print("  âŒ Valeur invalide (ex: /topk 50)")
                    
                    elif cmd == "/tokens" and len(parts) > 1:
                        try:
                            self.max_tokens = int(parts[1])
                            print(f"  Max tokens â†’ {self.max_tokens}")
                        except:
                            print("  âŒ Valeur invalide (ex: /tokens 100)")
                    
                    else:
                        print("  âŒ Commande inconnue. Tape /help pour l'aide.")
                    
                    continue
                
                # GÃ©nÃ©ration
                print("\nğŸ¤– LLM: ", end="", flush=True)
                
                if streaming:
                    # Mode streaming (token par token)
                    first = True
                    for token in self.generate_stream(user_input):
                        if first:
                            first = False
                            continue  # Skip le prompt
                        print(token, end="", flush=True)
                    print("\n")
                else:
                    # Mode normal
                    output = self.generate(user_input)
                    print(output + "\n")
            
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Ã€ bientÃ´t!")
                break
            
            except Exception as e:
                print(f"\nâŒ Erreur: {e}\n")


def main():
    parser = argparse.ArgumentParser(description="Chat avec ton LLM")
    parser.add_argument(
        "--model", "-m",
        type=str,
        default="french",
        help="ModÃ¨le Ã  charger: 'french', 'english', ou chemin vers un fichier .pt"
    )
    args = parser.parse_args()
    
    # Device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Trouver le modÃ¨le
    checkpoint_dir = Path(__file__).parent.parent / "checkpoints"
    
    if args.model == "french":
        model_path = checkpoint_dir / "best_model_french.pt"
    elif args.model == "english":
        model_path = checkpoint_dir / "best_model.pt"
    else:
        model_path = Path(args.model)
    
    # VÃ©rifier que le modÃ¨le existe
    if not model_path.exists():
        print(f"âŒ ModÃ¨le non trouvÃ©: {model_path}")
        print("\nModÃ¨les disponibles:")
        for f in checkpoint_dir.glob("*.pt"):
            print(f"  - {f.name}")
        print("\nUsage:")
        print("  python chat.py --model french")
        print("  python chat.py --model english")
        print("  python chat.py --model chemin/vers/model.pt")
        return
    
    # Lancer le chat
    chat = TerminalChat(str(model_path), device)
    chat.run()


if __name__ == "__main__":
    main()